{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, we take a look at:\n",
    "\n",
    "* Embedded data: we run inference with pretrained encoders to construct the concatenation of chunk embeddings\n",
    "* Map Graph construction: for imaging data, we need to also convert the embedded data to map graphs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding data -- DO LATER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is something we only have to perform for data that's not natively graphs. We use `networkx` to create our data structures. \n",
    "\n",
    "Let's now load and set up model $f_\\theta$. Choices for Camelyon16 data include:\n",
    "- `\"tile2vec\"`: an unsupervised learning model, ResNet-16 trained from scratch\n",
    "- `\"vit_iid\"`: a (weakly) supervised learning model, ViT trained from scratch on IID fuzzy targets\n",
    "- `\"clip\"`: a Foundation Model, specifically a Vision-Langauge Model (VLM), pre-trained and used out of the box\n",
    "- `\"plip\"`: a Foundation Model/VLM, pre-trained and used out of the box; clip-style model that is fine-tuned on Patholgy chunks\n",
    "- `None`: skip inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelstr = \"plip\" #\"tile2vec\", \"vit_iid\", \"clip\", \"plip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if modelstr == \"tile2vec\":\n",
    "#     from architectures import ResNet18 \n",
    "#     model = ResNet18(n_classes=2, in_channels=3, z_dim=128, supervised=False, no_relu=False, loss_type='triplet', tile_size=224, activation='relu')\n",
    "#     chkpt = \"/home/lofi/lofi/models/cam/to-port/ResNet18-hdf5_triplets_random_loading-224-label_selfsup-custom_loss-on_cam-cam16-filtration_background.sd\"\n",
    "#     checkpoint = torch.load(chkpt, map_location=device)\n",
    "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#     model.to(device)\n",
    "#     prev_epoch = checkpoint['epoch']\n",
    "#     loss = checkpoint['loss']\n",
    "# elif modelstr == \"vit_iid\":\n",
    "#     from vit_pytorch import ViT\n",
    "#     model = ViT(image_size = 224, patch_size=16, num_classes=2, dim=1024, depth=6, heads=16, mlp_dim=2048, dropout=0.1, emb_dropout=0.1)\n",
    "#     chkpt = \"/home/lofi/lofi/models/cam/ViT-hdf5_random_loading-224-label_inherit-bce_loss-on_cam-cam16-filtration_background.sd\"\n",
    "#     checkpoint = torch.load(chkpt, map_location=device)\n",
    "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#     model.to(device)\n",
    "#     prev_epoch = checkpoint['epoch']\n",
    "#     loss = checkpoint['loss']\n",
    "# elif modelstr == \"clip\":\n",
    "#     from transformers import CLIPProcessor, CLIPTokenizer, CLIPModel\n",
    "#     tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "#     processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "#     model_clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "# elif modelstr == \"plip\":\n",
    "#     from transformers import AutoProcessor, AutoTokenizer, AutoModelForZeroShotImageClassification\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(\"vinid/plip\")\n",
    "#     processor = AutoProcessor.from_pretrained(\"vinid/plip\")\n",
    "#     model_plip = AutoModelForZeroShotImageClassification.from_pretrained(\"vinid/plip\")\n",
    "# elif modelstr == None:\n",
    "#     print(\"No model selected for inference! Skipping inference...\")\n",
    "# else:\n",
    "#     print(\"Not yet supported for inference! Skipping inference...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting embedded data to map graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import utils\n",
    "import networkx as nx\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelstr = \"plip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if modelstr == \"tile2vec\":\n",
    "    Z_dir = \"/home/data/tinycam/train/Zs_tile2vec\"\n",
    "    save_dir = \"/home/data/tinycam/train/Gs_tile2vec\"\n",
    "elif modelstr == \"vit_iid\":\n",
    "    Z_dir = \"/home/data/tinycam/train/Zs_vit\"\n",
    "    save_dir = \"/home/data/tinycam/train/Gs_vit_iid\"\n",
    "elif modelstr == \"clip\":\n",
    "    Z_dir = \"/home/data/tinycam/train/Zs_clip\"\n",
    "    save_dir = \"/home/data/tinycam/train/Gs_clip\"\n",
    "elif modelstr == \"plip\":\n",
    "    Z_dir = \"/home/data/tinycam/train/Zs_plip\"\n",
    "    save_dir = \"/home/data/tinycam/train/Gs_plip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Z_obj in os.listdir(Z_dir):\n",
    "#     sample_id = Z_obj.split(\".npy\")[0]\n",
    "#     G_id = \"G-\" + sample_id.split(\"-\")[1]\n",
    "\n",
    "#     print(\"converting {}\".format(Z_obj))\n",
    "#     Z_path = str(os.path.join(Z_dir, Z_obj))\n",
    "#     Z = np.load(Z_path)\n",
    "#     G = utils.convert_arr2graph(Z)\n",
    "#     save_path = os.path.join(save_dir, G_id)\n",
    "#     utils.serialize(G, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {}\n",
    "save_dir = \"/home/data/tinycam/train/\"\n",
    "G_dir = \"/home/data/tinycam/train/Gs_\" + modelstr\n",
    "\n",
    "for G_obj in os.listdir(G_dir):\n",
    "    sample_id = G_obj.split(\".npy\")[0]\n",
    "    G_id = \"G-\" + sample_id.split(\"-\")[1]\n",
    "    if \"normal\" in G_id:\n",
    "        label_dict[G_id] = 0\n",
    "    else: # tumor\n",
    "        label_dict[G_id] = 1\n",
    "    \n",
    "utils.serialize(label_dict, os.path.join(save_dir, modelstr+\"-label_dict.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean test set embedded images\n",
    "- do later, draw from previous repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert test images to graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if modelstr == \"tile2vec\":\n",
    "    Z_dir = \"/home/data/tinycam/test/clean_Zs_tile2vec\"\n",
    "    save_dir = \"/home/data/tinycam/test/Gs_tile2vec\"\n",
    "elif modelstr == \"vit_iid\":\n",
    "    Z_dir = \"/home/data/tinycam/test/clean_Zs_vit_iid\"\n",
    "    save_dir = \"/home/data/tinycam/test/Gs_vit_iid\"\n",
    "elif modelstr == \"clip\":\n",
    "    Z_dir = \"/home/data/tinycam/test/clean_Zs_clip\"\n",
    "    save_dir = \"/home/data/tinycam/test/Gs_clip\"\n",
    "elif modelstr == \"plip\":\n",
    "    Z_dir = \"/home/data/tinycam/test/clean_Zs_plip\"\n",
    "    save_dir = \"/home/data/tinycam/test/Gs_plip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting Z-test_129.npy\n",
      "converting Z-test_128.npy\n",
      "converting Z-test_001.npy\n",
      "converting Z-test_087.npy\n",
      "converting Z-test_067.npy\n",
      "converting Z-test_009.npy\n",
      "converting Z-test_100.npy\n",
      "converting Z-test_091.npy\n",
      "converting Z-test_040.npy\n",
      "converting Z-test_003.npy\n",
      "converting Z-test_076.npy\n",
      "converting Z-test_022.npy\n",
      "converting Z-test_035.npy\n",
      "converting Z-test_017.npy\n",
      "converting Z-test_093.npy\n",
      "converting Z-test_039.npy\n",
      "converting Z-test_066.npy\n",
      "converting Z-test_122.npy\n",
      "converting Z-test_012.npy\n",
      "converting Z-test_057.npy\n",
      "converting Z-test_032.npy\n",
      "converting Z-test_061.npy\n",
      "converting Z-test_082.npy\n",
      "converting Z-test_108.npy\n",
      "converting Z-test_038.npy\n",
      "converting Z-test_106.npy\n",
      "converting Z-test_030.npy\n",
      "converting Z-test_080.npy\n",
      "converting Z-test_058.npy\n",
      "converting Z-test_075.npy\n",
      "converting Z-test_096.npy\n",
      "converting Z-test_127.npy\n",
      "converting Z-test_004.npy\n",
      "converting Z-test_056.npy\n",
      "converting Z-test_068.npy\n",
      "converting Z-test_054.npy\n",
      "converting Z-test_110.npy\n",
      "converting Z-test_036.npy\n",
      "converting Z-test_048.npy\n",
      "converting Z-test_070.npy\n",
      "converting Z-test_021.npy\n",
      "converting Z-test_024.npy\n",
      "converting Z-test_044.npy\n",
      "converting Z-test_011.npy\n",
      "converting Z-test_033.npy\n",
      "converting Z-test_006.npy\n",
      "converting Z-test_025.npy\n",
      "converting Z-test_116.npy\n",
      "converting Z-test_085.npy\n",
      "converting Z-test_112.npy\n",
      "converting Z-test_013.npy\n",
      "converting Z-test_010.npy\n",
      "converting Z-test_027.npy\n",
      "converting Z-test_114.npy\n",
      "converting Z-test_028.npy\n",
      "converting Z-test_117.npy\n",
      "converting Z-test_018.npy\n",
      "converting Z-test_063.npy\n",
      "converting Z-test_047.npy\n",
      "converting Z-test_109.npy\n",
      "converting Z-test_119.npy\n",
      "converting Z-test_050.npy\n",
      "converting Z-test_005.npy\n",
      "converting Z-test_053.npy\n",
      "converting Z-test_037.npy\n",
      "converting Z-test_065.npy\n",
      "converting Z-test_130.npy\n",
      "converting Z-test_090.npy\n",
      "converting Z-test_088.npy\n",
      "converting Z-test_072.npy\n",
      "converting Z-test_103.npy\n",
      "converting Z-test_111.npy\n",
      "converting Z-test_046.npy\n",
      "converting Z-test_002.npy\n",
      "converting Z-test_126.npy\n",
      "converting Z-test_069.npy\n",
      "converting Z-test_077.npy\n",
      "converting Z-test_014.npy\n",
      "converting Z-test_118.npy\n",
      "converting Z-test_023.npy\n",
      "converting Z-test_042.npy\n",
      "converting Z-test_083.npy\n",
      "converting Z-test_078.npy\n",
      "converting Z-test_073.npy\n",
      "converting Z-test_034.npy\n",
      "converting Z-test_121.npy\n",
      "converting Z-test_015.npy\n",
      "converting Z-test_045.npy\n",
      "converting Z-test_060.npy\n",
      "converting Z-test_059.npy\n",
      "converting Z-test_089.npy\n",
      "converting Z-test_113.npy\n",
      "converting Z-test_051.npy\n",
      "converting Z-test_020.npy\n",
      "converting Z-test_071.npy\n",
      "converting Z-test_124.npy\n",
      "converting Z-test_019.npy\n",
      "converting Z-test_026.npy\n",
      "converting Z-test_115.npy\n",
      "converting Z-test_031.npy\n",
      "converting Z-test_064.npy\n",
      "converting Z-test_125.npy\n",
      "converting Z-test_123.npy\n",
      "converting Z-test_092.npy\n",
      "converting Z-test_008.npy\n",
      "converting Z-test_102.npy\n",
      "converting Z-test_081.npy\n",
      "converting Z-test_041.npy\n",
      "converting Z-test_079.npy\n",
      "converting Z-test_052.npy\n",
      "converting Z-test_101.npy\n",
      "converting Z-test_104.npy\n",
      "converting Z-test_007.npy\n",
      "converting Z-test_074.npy\n",
      "converting Z-test_107.npy\n",
      "converting Z-test_099.npy\n",
      "converting Z-test_120.npy\n",
      "converting Z-test_105.npy\n",
      "converting Z-test_029.npy\n",
      "converting Z-test_094.npy\n",
      "converting Z-test_097.npy\n",
      "converting Z-test_098.npy\n",
      "converting Z-test_055.npy\n",
      "converting Z-test_062.npy\n",
      "converting Z-test_095.npy\n",
      "converting Z-test_084.npy\n",
      "converting Z-test_016.npy\n",
      "converting Z-test_043.npy\n",
      "converting Z-test_086.npy\n"
     ]
    }
   ],
   "source": [
    "for Z_obj in os.listdir(Z_dir):\n",
    "    sample_id = Z_obj.split(\".npy\")[0]\n",
    "    G_id = \"G-\" + sample_id.split(\"-\")[1]\n",
    "\n",
    "    print(\"converting {}\".format(Z_obj))\n",
    "    Z_path = str(os.path.join(Z_dir, Z_obj))\n",
    "    Z = np.load(Z_path)\n",
    "    G = utils.convert_arr2graph(Z)\n",
    "    save_path = os.path.join(save_dir, G_id)\n",
    "    utils.serialize(G, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
