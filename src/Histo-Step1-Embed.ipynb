{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, we take a look at:\n",
    "\n",
    "* Embedded data: we run inference with pretrained encoders to construct the concatenation of chunk embeddings\n",
    "* Map Graph construction: for imaging data, we need to also convert the embedded data to map graphs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding data -- DO LATER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is something we only have to perform for data that's not natively graphs. We use `networkx` to create our data structures. \n",
    "\n",
    "Let's now load and set up model $f_\\theta$. Choices for Camelyon16 data include:\n",
    "- `\"tile2vec\"`: an unsupervised learning model, ResNet-16 trained from scratch\n",
    "- `\"vit_iid\"`: a (weakly) supervised learning model, ViT trained from scratch on IID fuzzy targets\n",
    "- `\"clip\"`: a Foundation Model, specifically a Vision-Langauge Model (VLM), pre-trained and used out of the box\n",
    "- `\"plip\"`: a Foundation Model/VLM, pre-trained and used out of the box; clip-style model that is fine-tuned on Patholgy chunks\n",
    "- `None`: skip inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelstr = \"plip\" #\"tile2vec\", \"vit_iid\", \"clip\", \"plip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if modelstr == \"tile2vec\":\n",
    "#     from architectures import ResNet18 \n",
    "#     model = ResNet18(n_classes=2, in_channels=3, z_dim=128, supervised=False, no_relu=False, loss_type='triplet', tile_size=224, activation='relu')\n",
    "#     chkpt = \"/home/lofi/lofi/models/cam/to-port/ResNet18-hdf5_triplets_random_loading-224-label_selfsup-custom_loss-on_cam-cam16-filtration_background.sd\"\n",
    "#     checkpoint = torch.load(chkpt, map_location=device)\n",
    "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#     model.to(device)\n",
    "#     prev_epoch = checkpoint['epoch']\n",
    "#     loss = checkpoint['loss']\n",
    "# elif modelstr == \"vit_iid\":\n",
    "#     from vit_pytorch import ViT\n",
    "#     model = ViT(image_size = 224, patch_size=16, num_classes=2, dim=1024, depth=6, heads=16, mlp_dim=2048, dropout=0.1, emb_dropout=0.1)\n",
    "#     chkpt = \"/home/lofi/lofi/models/cam/ViT-hdf5_random_loading-224-label_inherit-bce_loss-on_cam-cam16-filtration_background.sd\"\n",
    "#     checkpoint = torch.load(chkpt, map_location=device)\n",
    "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#     model.to(device)\n",
    "#     prev_epoch = checkpoint['epoch']\n",
    "#     loss = checkpoint['loss']\n",
    "# elif modelstr == \"clip\":\n",
    "#     from transformers import CLIPProcessor, CLIPTokenizer, CLIPModel\n",
    "#     tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "#     processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "#     model_clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "# elif modelstr == \"plip\":\n",
    "#     from transformers import AutoProcessor, AutoTokenizer, AutoModelForZeroShotImageClassification\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(\"vinid/plip\")\n",
    "#     processor = AutoProcessor.from_pretrained(\"vinid/plip\")\n",
    "#     model_plip = AutoModelForZeroShotImageClassification.from_pretrained(\"vinid/plip\")\n",
    "# elif modelstr == None:\n",
    "#     print(\"No model selected for inference! Skipping inference...\")\n",
    "# else:\n",
    "#     print(\"Not yet supported for inference! Skipping inference...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting embedded data to map graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import utils\n",
    "import networkx as nx\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelstr = \"tile2vec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if modelstr == \"tile2vec\":\n",
    "    Z_dir = \"/home/data/tinycam/train/Zs_tile2vec\"\n",
    "    save_dir = \"/home/data/tinycam/train/Gs_tile2vec\"\n",
    "elif modelstr == \"vit_iid\":\n",
    "    Z_dir = \"/home/data/tinycam/train/Zs_vit\"\n",
    "    save_dir = \"/home/data/tinycam/train/Gs_vit_iid\"\n",
    "elif modelstr == \"clip\":\n",
    "    Z_dir = \"/home/data/tinycam/train/Zs_clip\"\n",
    "    save_dir = \"/home/data/tinycam/train/Gs_clip\"\n",
    "elif modelstr == \"plip\":\n",
    "    Z_dir = \"/home/data/tinycam/train/Zs_plip\"\n",
    "    save_dir = \"/home/data/tinycam/train/Gs_plip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Z_obj in os.listdir(Z_dir):\n",
    "#     sample_id = Z_obj.split(\".npy\")[0]\n",
    "#     G_id = \"G-\" + sample_id.split(\"-\")[1]\n",
    "\n",
    "#     print(\"converting {}\".format(Z_obj))\n",
    "#     Z_path = str(os.path.join(Z_dir, Z_obj))\n",
    "#     Z = np.load(Z_path)\n",
    "#     G = utils.convert_arr2graph(Z)\n",
    "#     save_path = os.path.join(save_dir, G_id)\n",
    "#     utils.serialize(G, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {}\n",
    "save_dir = \"/home/data/tinycam/train/\"\n",
    "G_dir = \"/home/data/tinycam/train/Gs_\" + modelstr\n",
    "\n",
    "for G_obj in os.listdir(G_dir):\n",
    "    sample_id = G_obj.split(\".npy\")[0]\n",
    "    G_id = \"G-\" + sample_id.split(\"-\")[1]\n",
    "    if \"normal\" in G_id:\n",
    "        label_dict[G_id] = 0\n",
    "    else: # tumor\n",
    "        label_dict[G_id] = 1\n",
    "    \n",
    "utils.serialize(label_dict, os.path.join(save_dir, modelstr+\"-label_dict.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
