{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_selection import top_model_confusion\n",
    "from utils import serialize, deserialize, serialize_model, deserialize_model\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "encoder = 'COLLAPSE'\n",
    "metal = 'ZN'\n",
    "metric_str = \"precision\"\n",
    "results_cache_dir = f\"../data/{encoder}_{metal}_gridsearch_results/{encoder}-eval_results\"\n",
    "model_cache_dir = f\"../data/{encoder}_{metal}_gridsearch_results/{encoder}-fitted_k2_models\"\n",
    "linearized_cache_dir = f\"../data/{encoder}_{metal}_gridsearch_results/{encoder}-linearized_data\"\n",
    "\n",
    "# top_model_confusion(metric_str,results_cache_dir, model_cache_dir, eval_class=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_all_models = deserialize(f\"../data/results/{metal}_confusion_all_train_models.pkl\")\n",
    "confusion_class1_models = deserialize(f\"../data/results/{metal}_confusion_class1_train_models.pkl\")\n",
    "datum_level_models = deserialize(f\"../data/results/{metal}_datum_level_train_models.pkl\")\n",
    "continuous_avg_models = deserialize(f\"../data/results/{metal}_continuous_avg_train_models.pkl\")\n",
    "continuous_iid_models = deserialize(f\"../data/results/{metal}_continuous_iid_train_models.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_class1_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_selection import k_hop_precision\n",
    "for encoder in ['COLLAPSE', 'ESM', 'AA']:\n",
    "    results_cache_dir = f\"../data/{encoder}_{metal}_gridsearch_results/{encoder}-eval_results\"\n",
    "    model_cache_dir = f\"../data/{encoder}_{metal}_gridsearch_results/{encoder}-fitted_k2_models\"\n",
    "    linearized_cache_dir = f\"../data/{encoder}_{metal}_gridsearch_results/{encoder}-linearized_data\"\n",
    "    graph_base_dir = f\"../data/COLLAPSE_{metal}_cutoff_train_graphs\"\n",
    "\n",
    "    results = k_hop_precision('protein', results_cache_dir, linearized_cache_dir, model_cache_dir, graph_base_dir, eval_class='both')\n",
    "    \n",
    "    print(encoder, results)\n",
    "    \"\"\"\n",
    "    \n",
    "for encoder in ['COLLAPSE', 'ESM', 'AA']:\n",
    "    results_cache_dir = f\"../data/{encoder}_{metal}_gridsearch_results/{encoder}-eval_results\"\n",
    "    model_cache_dir = f\"../data/{encoder}_{metal}_gridsearch_results/{encoder}-fitted_k2_models\"\n",
    "    linearized_cache_dir = f\"../data/{encoder}_{metal}_gridsearch_results/{encoder}-linearized_data\"\n",
    "    graph_base_dir = f\"../data/{encoder}_{metal}_cutoff_train_graphs\"\n",
    "\n",
    "    results = k_hop_precision('protein', results_cache_dir, linearized_cache_dir, model_cache_dir, graph_base_dir, eval_class=1)\n",
    "    print(encoder, results)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_metrics = [\"msd\", \"specificity\", \"precision\", \"fnr\", \"fdr\", \"recall\", \"accuracy\", \"balanced_acc\", \"correlation\", \"threat_score\", \"prevalence\", \"dice\", \"jaccard\"]\n",
    "for metric_str in valid_metrics:\n",
    "    print(metric_str)\n",
    "    results = top_model_confusion(metric_str,results_cache_dir, model_cache_dir)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_metrics = [\"msd\", \"specificity\", \"precision\", \"fnr\", \"fdr\", \"recall\", \"accuracy\", \"balanced_acc\", \"correlation\", \"threat_score\", \"prevalence\", \"dice\", \"jaccard\"]\n",
    "for metric_str in valid_metrics:\n",
    "    print(metric_str)\n",
    "    results = top_model_confusion(metric_str, results_cache_dir, model_cache_dir, eval_class=0)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_metrics = [\"msd\", \"specificity\", \"precision\", \"fnr\", \"fdr\", \"recall\", \"accuracy\", \"balanced_acc\", \"correlation\", \"threat_score\", \"prevalence\", \"dice\", \"jaccard\"]\n",
    "for metric_str in valid_metrics:\n",
    "    print(metric_str)\n",
    "    results = top_model_confusion(metric_str, results_cache_dir, model_cache_dir, eval_class=1)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_selection import top_model_preds\n",
    "valid_metrics = [\"auroc\", \"auprc\", \"ap\"]\n",
    "for metric_str in valid_metrics:\n",
    "    print(metric_str)\n",
    "    results = top_model_preds(metric_str, results_cache_dir, model_cache_dir)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_selection import top_model_continuous_avg\n",
    "valid_metrics = [\"auroc\", \"auprc\", \"ap\"]\n",
    "for metric_str in valid_metrics:\n",
    "    print(metric_str)\n",
    "    results = top_model_continuous_avg(metric_str, results_cache_dir, model_cache_dir)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "from model_selection import top_model_continuous_iid\n",
    "valid_metrics = [\"auroc\", \"auprc\", \"ap\"]\n",
    "for metric_str in valid_metrics:\n",
    "    print(metric_str)\n",
    "    results = top_model_continuous_iid(metric_str, model_cache_dir, linearized_cache_dir)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking gridsearch models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_conf_metrics = [\"precision\", \"correlation\", \"dice\"]\n",
    "key_cont_metrics = [\"ap\", \"auroc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import glob\n",
    "from evaluation import get_label, check_eval_metric\n",
    "valid_metrics = [\"msd\", \"specificity\", \"precision\", \"fnr\", \"fdr\", \"recall\", \"accuracy\", \"balanced_acc\", \"correlation\", \"threat_score\", \"prevalence\", \"dice\", \"jaccard\"]\n",
    "\n",
    "encoder = 'COLLAPSE'\n",
    "baseline = 'Attention'\n",
    "eval_class = 1\n",
    "metric_str = 'precision'\n",
    "metric = check_eval_metric(metric_str, valid_metrics)\n",
    "\n",
    "model_results_dict = utils.deserialize(f'../data/baselines/{encoder}_{baseline}_test_results.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_dict = {}\n",
    "all_scores = {}\n",
    "stabilities = []\n",
    "for path in glob.glob(f'../data/baselines/*_test_results.pkl'):\n",
    "    model_results_dict = utils.deserialize(path)\n",
    "    model_str = path.split('/')[-1].split('.')[0]\n",
    "    N=0\n",
    "    for graph_id in model_results_dict.keys():\n",
    "        datum_results_dict = model_results_dict[graph_id]\n",
    "        if eval_class in [0,1]:\n",
    "            y = get_label(datum_results_dict)\n",
    "            if y != eval_class:\n",
    "                continue\n",
    "        if metric_str == \"msd\":\n",
    "            datum_cms = datum_results_dict[\"thresh_msd\"]\n",
    "        else:\n",
    "            datum_cms = datum_results_dict[\"thresh_cm\"]\n",
    "        \n",
    "        model_cms = {} \n",
    "        for thresh in datum_cms.keys():\n",
    "            if type(thresh) == tuple:\n",
    "                new_thresh = thresh[0]\n",
    "                model_cms[new_thresh] = 0.0\n",
    "            else:\n",
    "                model_cms[thresh] = 0.0\n",
    "\n",
    "        for thresh in datum_cms.keys():\n",
    "            if type(thresh) == tuple:\n",
    "                new_thresh = thresh[0]\n",
    "                model_cms[new_thresh] += metric(datum_cms[thresh])\n",
    "            else:\n",
    "                model_cms[thresh] += metric(datum_cms[thresh])\n",
    "        N += 1\n",
    "            \n",
    "    # now average over all graphs\n",
    "    for thresh in model_cms.keys():\n",
    "        model_cms[thresh] /= N\n",
    "    # get top score and threshold\n",
    "    scores = [(thresh, model_cms[thresh]) for thresh in model_cms.keys()]\n",
    "    \n",
    "    max_score = max(scores, key=lambda item: item[1])\n",
    "    min_score = min(scores, key=lambda item: item[1])\n",
    "    stability = max_score[1] - min_score[1]\n",
    "    stabilities.append(stability)\n",
    "    metric_dict[model_str] = (max_score[0], max_score[1], stability)\n",
    "    all_scores[model_str] = scores\n",
    "    \n",
    "data = []\n",
    "for model_str, scores in all_scores.items():\n",
    "    for thresh, score in scores:\n",
    "        data.append([model_str, thresh, score])\n",
    "baseline_df = pd.DataFrame(data, columns=['model_name', 'threshold', 'score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_df.sort_values('score', ascending=False).groupby('model_name').head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prospectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "encoder = 'COLLAPSE'\n",
    "\n",
    "results_cache_dir = f\"../data/{encoder}_{metal}_gridsearch_results/{encoder}-eval_results\"\n",
    "model_cache_dir = f\"../data/{encoder}_{metal}_gridsearch_results/{encoder}-fitted_k2_models\"\n",
    "processor_cache_dir = f\"../data/{encoder}_{metal}_gridsearch_results/{encoder}-fitted_k2_processors\"\n",
    "linearized_cache_dir = f\"../data/{encoder}_{metal}_gridsearch_results/{encoder}-linearized_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_selection import top_model_confusion, top_model_continuous_avg, top_model_continuous_iid\n",
    "# conf_res = []\n",
    "# for metric in key_conf_metrics:\n",
    "#     print(metric)\n",
    "#     res = top_model_confusion(metric,results_cache_dir, model_cache_dir, eval_class=1, return_all=True)\n",
    "#     res[\"metric\"] = [metric]*len(res)\n",
    "#     conf_res.append(res)\n",
    "# conf_res = pd.concat(conf_res)\n",
    "\n",
    "cont_res = []\n",
    "for metric in key_cont_metrics:\n",
    "    print(metric)\n",
    "    res = top_model_continuous_avg(metric, results_cache_dir, model_cache_dir, return_all=True)\n",
    "    res[\"metric\"] = [metric]*len(res)\n",
    "    cont_res.append(res)\n",
    "cont_res = pd.concat(cont_res)\n",
    "\n",
    "# iid_res = []\n",
    "# for metric in key_cont_metrics:\n",
    "#     print(metric)\n",
    "#     res = top_model_continuous_iid(metric, model_cache_dir, linearized_cache_dir, return_all=True)\n",
    "#     res[\"metric\"] = [metric]*len(res)\n",
    "#     iid_res.append(res)\n",
    "# iid_res = pd.concat(iid_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iid_res.sort_values('score', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(cont_res, x='score', y='num_zeros', hue='metric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(cont_res.pivot(index=['model_name', 'num_zeros'], columns='metric', values='score'), x='ap', y='auroc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numzero_dict = dict(zip(cont_res.model_name, cont_res.num_zeros))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_res.pivot(index=['model_name', 'num_zeros'], columns='metric', values='score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_res['num_zeros'] = conf_res['model_name'].map(numzero_dict)\n",
    "conf_pvt = conf_res.pivot(index=['model_name', 'threshold', 'num_zeros'], columns='metric', values='score')\n",
    "for met in key_conf_metrics:\n",
    "    rank = conf_pvt[met].rank(method='dense', ascending=False)\n",
    "    conf_pvt[f'rank_{met}'] = rank\n",
    "# conf_pvt['rank'] = conf_pvt[key_conf_metrics].apply(tuple,axis=1).rank(method='dense',ascending=False)\n",
    "\n",
    "cont_pvt = cont_res.pivot(index=['model_name', 'num_zeros'], columns='metric', values='score')\n",
    "cont_pvt['rank_auprc'] = cont_pvt['ap'].rank(method='dense', ascending=False)\n",
    "cont_pvt['rank_auroc'] = cont_pvt['auroc'].rank(method='dense', ascending=False)\n",
    "\n",
    "merged = pd.merge(conf_pvt.reset_index(level=['threshold', 'num_zeros']), cont_pvt, on='model_name', how='left')\n",
    "merged['rank_zeros'] = merged['num_zeros'].rank(method='dense', ascending=True)\n",
    "merged['avg_rank'] = merged[[f'rank_{i}' for i in key_conf_metrics] + ['rank_auprc'] + ['rank_zeros']].mean(axis=1)\n",
    "merged.sort_values('avg_rank').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.sort_values(['num_zeros', 'rank_auprc', 'rank_precision']).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[(~merged.index.str.contains('r0'))].sort_values(['rank_auprc', 'rank_precision'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.reset_index().sort_values('ap', ascending=False).groupby('model_name').head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data=merged.sort_values('ap').groupby('model_name').head(1), x='correlation', y='ap', hue='num_zeros')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_selection import extract_params\n",
    "copy = merged.copy().reset_index()\n",
    "copy[['k', 'r', 'cutoff', 'alpha', 'tau', 'lambda']] = copy.apply(lambda x: extract_params(x.model_name), axis='columns', result_type='expand')\n",
    "copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy[['num_zeros', 'k', 'r', 'cutoff', 'alpha', 'tau', 'lambda']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data=copy.sort_values('ap').groupby('model_name').head(1), x='correlation', y='ap', hue='r', size='num_zeros')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import gridsearch_iteration, fetch_model, fetch_processor\n",
    "from model_selection import extract_params\n",
    "\n",
    "k, r, cutoff, alpha, tau, lamb = extract_params('k15_r4_cutoff8.00_alpha0.0010_tau4.00_lamnan.model')\n",
    "\n",
    "process_args = {\"datatype\": \"protein\",\n",
    "            \"k\": None,\n",
    "            \"dataset\": 'ZN',\n",
    "            \"quantizer_type\": \"AA\" if encoder == \"AA\" else \"kmeans\",\n",
    "            \"embeddings_path\": f\"../data/{encoder}_ZN_{cutoff}_train_embeddings_2.pkl\",\n",
    "            \"embeddings_type\": \"dict\",\n",
    "            \"mapping_path\": None,\n",
    "            \"sample_size\": None,\n",
    "            \"sample_scheme\": None,\n",
    "            \"dataset_path\": None,\n",
    "            \"verbosity\": \"low\", # change this to low!\n",
    "            \"so_dict_path\": None}\n",
    "\n",
    "proc, processor_name = fetch_processor(k, processor_cache_dir, process_args, cutoff=cutoff)\n",
    "hparams = {\"alpha\": alpha, \"tau\": tau, \"lambda\": lamb}\n",
    "model_args = {\"modality\":\"graph\",\n",
    "        \"processor\": proc,\n",
    "        \"r\": r,\n",
    "        \"variant\": \"inferential\",\n",
    "        \"hparams\": hparams,\n",
    "        \"train_graph_path\": f\"../data/{encoder}_ZN_{cutoff}_train_graphs_2\",\n",
    "        \"train_label_dict\": None}\n",
    "model, model_str = fetch_model(proc, r, model_cache_dir, model_args, cutoff=cutoff, alpha=alpha, tau=tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results_dict, data_linearized_dict = gridsearch_iteration(model, model_args, gt_dir=None, thresh=\"all\", arm=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_str = 'precision'\n",
    "eval_class = 1\n",
    "metric = check_eval_metric(metric_str, valid_metrics)\n",
    "N=0\n",
    "for graph_id in model_results_dict.keys():\n",
    "    datum_results_dict = model_results_dict[graph_id]\n",
    "    print\n",
    "    if eval_class in [0,1]:\n",
    "        y = get_label(datum_results_dict)\n",
    "        if y != eval_class:\n",
    "            continue\n",
    "    if metric_str == \"msd\":\n",
    "        datum_cms = datum_results_dict[\"thresh_msd\"]\n",
    "    else:\n",
    "        datum_cms = datum_results_dict[\"thresh_cm\"]\n",
    "    \n",
    "    model_cms = {} \n",
    "    for thresh in datum_cms.keys():\n",
    "        if type(thresh) == tuple:\n",
    "            new_thresh = thresh[0]\n",
    "            model_cms[new_thresh] = 0.0\n",
    "        else:\n",
    "            model_cms[thresh] = 0.0\n",
    "\n",
    "    for thresh in datum_cms.keys():\n",
    "        if type(thresh) == tuple:\n",
    "            new_thresh = thresh[0]\n",
    "            model_cms[new_thresh] += metric(datum_cms[thresh])\n",
    "        else:\n",
    "            model_cms[thresh] += metric(datum_cms[thresh])\n",
    "    N += 1\n",
    "        \n",
    "# now average over all graphs\n",
    "for thresh in model_cms.keys():\n",
    "    model_cms[thresh] /= N\n",
    "# get top score and threshold\n",
    "scores = [(thresh, model_cms[thresh]) for thresh in model_cms.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_precision = top_model_confusion(\"precision\",results_cache_dir, model_cache_dir, eval_class=1, return_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from model_selection import top_model_continuous_avg\n",
    "res_auc = top_model_continuous_avg('auprc', results_cache_dir, model_cache_dir, return_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "res_precision[['k', 'r', 'cutoff', 'alpha', 'tau', 'lambda']] = res_precision.apply(lambda x: extract_params(x.model_name), axis='columns', result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_auc[['k', 'r', 'cutoff', 'alpha', 'tau', 'lambda']] = res_auc.apply(lambda x: extract_params(x.model_name), axis='columns', result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.merge(res_precision, res_auc, on=['model_name', 'k', 'r', 'cutoff', 'alpha', 'tau', 'lambda'], how='inner', suffixes=('_precision', '_auprc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_inf = res.dropna(subset=['alpha', 'tau'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_inf['alpha'] = res_inf['alpha'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_inf = res_inf.sort_values('score_precision', ascending=False).groupby('model_name').nth(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data=res_inf, x='score_precision', y='score_auprc', hue='tau', size='cutoff', markers='alpha', palette='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data=res_inf, x='score_precision', y='score_auprc', hue='k', size='r', markers='alpha', palette='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data=res_inf, x='score_precision', y='score_auprc', hue='k', size='alpha', markers='alpha', palette='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_lin = res.dropna(subset=['lambda'])\n",
    "res_lin = res_lin.sort_values('score_precision', ascending=False).groupby('model_name').nth(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data=res_lin, x='score_precision', y='score_auprc', hue='k', size='r', markers='alpha', palette='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_lin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hard coded for now from looking at top of ranked dataframe\n",
    "encoder_top_models = \\\n",
    "    {'COLLAPSE': ('k15_r1_cutoff8.00_alpha0.500_tau4.00_lamnan.model', 0.9), \\\n",
    "     # {'COLLAPSE': ('k20_r0_cutoff4.00_alpha0.001_tau0.00_lamnan.model', 0.9), \\\n",
    "    'ESM': ('k30_r1_cutoff4.00_alpha0.500_tau1.00_lamnan.model', 0.0), \\\n",
    "    'AA': ('k21_r2_cutoff6.00_alphanan_taunan_lam1.00.model', 0.5)}\n",
    "\n",
    "baseline_top_models = \\\n",
    "    {'COLLAPSE': ('COLLAPSE-ZN-8.0-0.0005', 0.7), \\\n",
    "    'ESM': ('ESM-ZN-8.0-0.0005', 0.4), \\\n",
    "    'AA': ('AA-ZN-8.0-0.0005', 0.6)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_conf_metrics = [\"precision\", \"correlation\", \"dice\"]\n",
    "key_cont_metrics = [\"auprc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import test_eval, extract_params\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \"is_categorical_dtype\")\n",
    "warnings.filterwarnings(\"ignore\", \"use_inf_as_na\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_figure(width=6, height=3):\n",
    "    sns.set(style='white')\n",
    "    sns.set_context('paper')\n",
    "    plt.figure(figsize=(width,height))\n",
    "pal = sns.color_palette('tab20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metal = 'ZN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = []\n",
    "test_metrics = key_conf_metrics + key_cont_metrics\n",
    "for encoder, (model_str, threshold) in encoder_top_models.items():\n",
    "    results_cache_dir = f\"../data/{encoder}_{metal}_gridsearch_results_2/{encoder}-eval_results\"\n",
    "    model_cache_dir = f\"../data/{encoder}_{metal}_gridsearch_results_2/{encoder}-fitted_k2_models\"\n",
    "    processor_cache_dir = f\"../data/{encoder}_{metal}_gridsearch_results_2/{encoder}-fitted_k2_processors\"\n",
    "    linearized_cache_dir = f\"../data/{encoder}_{metal}_gridsearch_results_2/{encoder}-linearized_data\"\n",
    "\n",
    "    _,_,cutoff,_,_,_ = extract_params(model_str)\n",
    "\n",
    "    if encoder == 'AA':\n",
    "        g_encoder = 'COLLAPSE'\n",
    "    else:\n",
    "        g_encoder = encoder\n",
    "\n",
    "    G_dir = f\"../data/{g_encoder}_{metal}_{cutoff}_test_graphs_2\"\n",
    "    \n",
    "    df = test_eval(model_str, threshold, test_metrics, model_cache_dir, processor_cache_dir, G_dir, gt_dir=None, label_dict=None, modality=\"graph\")\n",
    "    test_df.append(df)\n",
    "test_df = pd.concat(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baselines\n",
    "from evaluation import get_test_metrics\n",
    "\n",
    "base_df = []\n",
    "for encoder in ['COLLAPSE', 'ESM', 'AA']:\n",
    "    best_model, best_thresh = baseline_top_models[encoder]\n",
    "    results_dict = deserialize(f'../data/baselines/{encoder}_test_results.pkl')\n",
    "    df = get_test_metrics(results_dict, encoder, best_model, best_thresh, test_metrics)\n",
    "    base_df.append(df)\n",
    "base_df = pd.concat(base_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([test_df, base_df])\n",
    "combined_df['method'] = ['Prospector']*len(test_df) + ['GAT+GNNExplainer']*len(base_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('../data/results/K2_test_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for met in test_metrics:\n",
    "    subdf = combined_df[combined_df.metric == met].reset_index()\n",
    "\n",
    "    plt.clf()\n",
    "    if met == 'auprc':\n",
    "        setup_figure(6,3)\n",
    "        ax = sns.barplot(data=subdf[subdf.regime == 'all'], x='value', y='encoder', hue='method', orient='horizontal', errorbar='se', capsize=0.05, errwidth=1.0, linewidth=1, edgecolor=\"k\")\n",
    "        sns.stripplot(data=subdf[subdf.regime == 'all'], x='value', y='encoder',  hue='method', orient='horizontal', dodge=True, alpha=0.1, linewidth=0.5, ax=ax, legend=False)\n",
    "        plt.title(met, fontsize=12)\n",
    "        plt.legend(loc=(0.65,1.01))\n",
    "    else:\n",
    "        fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(6, 3), sharey=True, gridspec_kw={'wspace': 0})\n",
    "        sns.barplot(data=subdf[subdf['regime'] == 'class-1'], x='value', y='encoder', hue='method', orient='horizontal', dodge=True, ax=ax2, errorbar='se', capsize=0.05, errwidth=1.0, linewidth=1, edgecolor=\"k\")\n",
    "        sns.stripplot(data=subdf[subdf['regime'] == 'class-1'], x='value', y='encoder', hue='method', orient='horizontal', dodge=True, alpha=0.1, linewidth=0.5, ax=ax2, legend=False)\n",
    "        # ax1.yaxis.set_label_position('left')\n",
    "\n",
    "        ax2.set_title('  '+'class-1', loc='left')\n",
    "        ax2.set_ylabel('')\n",
    "        ax2.set_yticklabels([])\n",
    "        ax2.legend_.remove()\n",
    "    \n",
    "        sns.barplot(data=subdf[subdf['regime'] == 'all'], x='value', y='encoder', hue='method', orient='horizontal', dodge=True, ax=ax1, errorbar='se', capsize=0.05, errwidth=1.0, linewidth=1, edgecolor=\"k\")\n",
    "        sns.stripplot(data=subdf[subdf['regime'] == 'all'], x='value', y='encoder', hue='method', orient='horizontal', dodge=True, alpha=0.1, linewidth=0.5, ax=ax1, legend=False)\n",
    "        ax1.legend_.remove()\n",
    "    \n",
    "        # optionally use the same scale left and right\n",
    "        xmax = max(ax1.get_xlim()[1], ax2.get_xlim()[1])\n",
    "        ax1.set_xlim(xmax=xmax)\n",
    "        ax2.set_xlim(xmax=xmax)\n",
    "\n",
    "        ax1.invert_xaxis()  # reverse the direction\n",
    "        ax1.tick_params(axis='y', labelleft=True, left=True, labelright=False, right=False)\n",
    "        ax1.set_ylabel('')\n",
    "        ax1.set_title('all data'+'  ', loc='right')\n",
    "\n",
    "        plt.legend(loc=(-1.01,1.02))\n",
    "\n",
    "    fig.suptitle(met, fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(f'../data/figures/{met}-k2-vs-baseline.png', dpi=300, format='png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "k2",
   "language": "python",
   "name": "k2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
